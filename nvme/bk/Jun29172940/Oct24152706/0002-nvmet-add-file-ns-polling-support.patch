From 40953dc845905bf9ddc8354c72196e3e3ad8a472 Mon Sep 17 00:00:00 2001
From: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date: Thu, 24 Oct 2019 15:25:29 -0700
Subject: [PATCH 2/2] nvmet: add file-ns polling support

first time code is working whole verify

Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
---
 drivers/nvme/target/core.c        |   7 +-
 drivers/nvme/target/io-cmd-file.c | 199 ++++++++++++++++++++++++++++--
 drivers/nvme/target/nvmet.h       |   5 +
 3 files changed, 202 insertions(+), 9 deletions(-)

diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 05a94f33aea8..116fd18053c0 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -1457,11 +1457,16 @@ void nvmet_subsys_put(struct nvmet_subsys *subsys)
 
 static int nvmet_start_polling(void)
 {
-	return nvmet_bdev_start_polling();
+	/* TODO : fix error handling */
+	nvmet_bdev_start_polling();
+	nvmet_file_start_polling();
+	return 0;
 }
 
 static void nvmet_stop_polling(void)
 {
+	/* TODO : fix error handling */
+	nvmet_file_stop_polling();
 	nvmet_bdev_stop_polling();
 }
 
diff --git a/drivers/nvme/target/io-cmd-file.c b/drivers/nvme/target/io-cmd-file.c
index 05453f5d1448..c53710c1b11b 100644
--- a/drivers/nvme/target/io-cmd-file.c
+++ b/drivers/nvme/target/io-cmd-file.c
@@ -8,11 +8,69 @@
 #include <linux/uio.h>
 #include <linux/falloc.h>
 #include <linux/file.h>
+#include <linux/sched/signal.h>
+#include <linux/kthread.h>
+
 #include "nvmet.h"
 
 #define NVMET_MAX_MPOOL_BVEC		16
 #define NVMET_MIN_MPOOL_OBJ		16
 
+static struct nvmet_poll_data *t;
+static int nvmet_file_poll_thread(void *data);
+
+void nvmet_file_stop_polling(void)
+{
+	int i;
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		if (!t[i].thread)
+			continue;
+
+		if (wq_has_sleeper(&t[i].poll_waitq))
+			wake_up(&t[i].poll_waitq);
+		kthread_park(t[i].thread);
+		kthread_stop(t[i].thread);
+	}
+}
+
+int nvmet_file_start_polling(void)
+{
+	int ret = 0;
+	int i;
+
+	t = kzalloc(sizeof(*t) * num_online_cpus(), GFP_KERNEL);
+	if (!t) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		init_completion(&t[i].thread_complete);
+		init_waitqueue_head(&t[i].poll_waitq);
+		INIT_LIST_HEAD(&t[i].list);
+		INIT_LIST_HEAD(&t[i].done);
+		mutex_init(&t[i].list_lock);
+
+		t[i].thread = kthread_create(nvmet_file_poll_thread, &t[i],
+					     "nvmet_poll_thread/%d", i);
+
+		if (IS_ERR(t[i].thread)) {
+			ret = PTR_ERR(t[i].thread);
+			goto out;
+		}
+		t[i].id = i;
+		kthread_bind(t[i].thread, i);
+	}
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		wake_up_process(t[i].thread);
+		wait_for_completion(&t[i].thread_complete);
+	}
+out:
+	return ret;
+}
+
 void nvmet_file_ns_disable(struct nvmet_ns *ns)
 {
 	if (ns->file) {
@@ -114,9 +172,8 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	return call_iter(iocb, &iter);
 }
 
-static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
+static void nvmet_file_req_complete(struct nvmet_req *req)
 {
-	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
 	u16 status = NVME_SC_SUCCESS;
 
 	if (req->f.bvec != req->inline_bvec) {
@@ -126,17 +183,110 @@ static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 			mempool_free(req->f.bvec, req->ns->bvec_pool);
 	}
 
-	if (unlikely(ret != req->data_len))
-		status = errno_to_nvme_status(req, ret);
+	if (unlikely(req->f.iosize != req->data_len))
+		status = errno_to_nvme_status(req, req->f.iosize);
 	nvmet_req_complete(req, status);
 }
 
+static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
+{
+	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
+
+	req->f.iosize = ret;
+	req->f.poll ? complete(&req->f.wait) : nvmet_file_req_complete(req);
+}
+
+static void nvmet_file_iopoll(struct nvmet_poll_data *t)
+{
+	struct nvmet_req *req, *tmp;
+	int ret;
+
+	lockdep_assert_held(&t->list_lock);
+
+	list_for_each_entry_safe(req, tmp, &t->list, poll_entry) {
+		if (completion_done(&req->f.wait)) {
+			list_move_tail(&req->poll_entry, &t->done);
+			continue;
+		}
+
+		if (!list_empty(&t->done) || need_resched())
+			break;
+
+		list_del(&req->poll_entry);
+		mutex_unlock(&t->list_lock);
+
+		while (!completion_done(&req->f.wait)) {
+			ret = req->f.iocb.ki_filp->f_op->iopoll(&req->f.iocb, true);
+			if (ret < 0)
+				pr_err("tid %d poll error %d", t->id, ret);
+			io_schedule();
+		}
+		/*
+		 * Since we are out of the lock anyway, by completing the
+		 * polled request here we reduces the lock contention and
+		 * decrease the size of done list which reduces the size of
+		 * list_lock. This allows nvmet_file_execute_rw() to make
+		 * progress as and when we scheduled out.
+		 */
+		nvmet_file_req_complete(req);
+		mutex_lock(&t->list_lock);
+	}
+
+	/*
+	 * In future we can also add batch timeout or nr request to complete.
+	 */
+	while (!list_empty(&t->done) && !need_resched()) {
+		/*
+		 * We lock and unlock for t->list which gurantee progress of
+		 * nvmet_file_rw_execute() when under pressure while we complete
+		 * the request.
+		 */
+		req = list_first_entry(&t->done, struct nvmet_req, poll_entry);
+		list_del(&req->poll_entry);
+		mutex_unlock(&t->list_lock);
+
+		nvmet_file_req_complete(req);
+
+		mutex_lock(&t->list_lock);
+	}
+}
+
+static int nvmet_file_poll_thread(void *data)
+{
+	struct nvmet_poll_data *t = (struct nvmet_poll_data *) data;
+	DEFINE_WAIT(wait);
+
+	complete(&t->thread_complete);
+
+	while (!kthread_should_park()) {
+
+		mutex_lock(&t->list_lock);
+		while (!list_empty(&t->list) && !need_resched())
+			nvmet_file_iopoll(t);
+		mutex_unlock(&t->list_lock);
+
+		prepare_to_wait(&t->poll_waitq, &wait, TASK_INTERRUPTIBLE);
+		if (signal_pending(current))
+			flush_signals(current);
+		smp_mb();
+		schedule();
+
+		finish_wait(&t->poll_waitq, &wait);
+	}
+
+	kthread_parkme();
+	return 0;
+}
+
 static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 {
+	struct kiocb *iocb = &req->f.iocb;
 	ssize_t nr_bvec = req->sg_cnt;
+	unsigned int tid = UINT_MAX;
 	unsigned long bv_cnt = 0;
 	bool is_sync = false;
 	size_t len = 0, total_len = 0;
+	unsigned long flags;
 	ssize_t ret = 0;
 	loff_t pos;
 	int i;
@@ -151,7 +301,7 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 		return true;
 	}
 
-	memset(&req->f.iocb, 0, sizeof(struct kiocb));
+	memset(iocb, 0, sizeof(struct kiocb));
 	for_each_sg(req->sg, sg, req->sg_cnt, i) {
 		nvmet_file_init_bvec(&req->f.bvec[bv_cnt], sg);
 		len += req->f.bvec[bv_cnt].bv_len;
@@ -187,13 +337,37 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 	 * A NULL ki_complete ask for synchronous execution, which we want
 	 * for the IOCB_NOWAIT case.
 	 */
-	if (!(ki_flags & IOCB_NOWAIT))
-		req->f.iocb.ki_complete = nvmet_file_io_done;
+	if (!(ki_flags & IOCB_NOWAIT)) {
+		iocb->ki_complete = nvmet_file_io_done;
+		if (req->ns->file->f_op->iopoll) {
+			local_irq_save(flags);
+			tid = smp_processor_id();
+			local_irq_restore(flags);
+			/*
+			 * This is to avoid race between poll thread and I/O
+			 * completion when req->f.wait is accessed in the
+			 * nvmet_file_io_done() and nvmet_file_io_poll_work().
+			 * We can only determine if this I/O is poll or not
+			 * based on return value of nvmet_file_submit_bvec().
+			 * (i.e. -EIOCBQUEUED)
+			 */
+			req->f.poll = true;
+			init_completion(&req->f.wait);
+			ki_flags |= IOCB_HIPRI;
+		}
+	}
 
 	ret = nvmet_file_submit_bvec(req, pos, bv_cnt, total_len, ki_flags);
 
 	switch (ret) {
 	case -EIOCBQUEUED:
+		if (req->f.poll) {
+			mutex_lock(&t[tid].list_lock);
+			list_add_tail(&req->poll_entry, &t[tid].list);
+			mutex_unlock(&t[tid].list_lock);
+			if (wq_has_sleeper(&t[tid].poll_waitq))
+				wake_up(&t[tid].poll_waitq);
+		}
 		return true;
 	case -EAGAIN:
 		if (WARN_ON_ONCE(!(ki_flags & IOCB_NOWAIT)))
@@ -211,7 +385,12 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 	}
 
 complete:
-	nvmet_file_io_done(&req->f.iocb, ret, 0);
+	/*
+	 * If we are here, then I/O is synchronously completed and ret holds
+	 * number of bytes transfered.
+	 */
+	req->f.iosize = ret;
+	nvmet_file_req_complete(req);
 	return true;
 }
 
@@ -370,19 +549,23 @@ u16 nvmet_file_parse_io_cmd(struct nvmet_req *req)
 	switch (cmd->common.opcode) {
 	case nvme_cmd_read:
 	case nvme_cmd_write:
+		req->f.poll = false;
 		req->execute = nvmet_file_execute_rw;
 		req->data_len = nvmet_rw_len(req);
 		return 0;
 	case nvme_cmd_flush:
+		req->f.poll = false;
 		req->execute = nvmet_file_execute_flush;
 		req->data_len = 0;
 		return 0;
 	case nvme_cmd_dsm:
+		req->f.poll = false;
 		req->execute = nvmet_file_execute_dsm;
 		req->data_len = (le32_to_cpu(cmd->dsm.nr) + 1) *
 			sizeof(struct nvme_dsm_range);
 		return 0;
 	case nvme_cmd_write_zeroes:
+		req->f.poll = false;
 		req->execute = nvmet_file_execute_write_zeroes;
 		req->data_len = 0;
 		return 0;
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index 5f5d64ea2ba5..f370b3651867 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -314,7 +314,10 @@ struct nvmet_req {
 			bool			mpool_alloc;
 			struct kiocb            iocb;
 			struct bio_vec          *bvec;
+			struct completion	wait;
 			struct work_struct      work;
+			long			iosize;
+			bool			poll;
 		} f;
 	};
 	int			sg_cnt;
@@ -461,6 +464,8 @@ void nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,
 
 int nvmet_bdev_start_polling(void);
 void nvmet_bdev_stop_polling(void);
+int nvmet_file_start_polling(void);
+void nvmet_file_stop_polling(void);
 
 #define NVMET_QUEUE_SIZE	1024
 #define NVMET_NR_QUEUES		128
-- 
2.22.1

