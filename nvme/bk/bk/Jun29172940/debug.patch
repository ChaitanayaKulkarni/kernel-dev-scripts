diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 6b39cfc6ade1..3bc67b8f21b7 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -7,6 +7,7 @@
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/rculist.h>
+#include <linux/kthread.h>
 #include <linux/pci-p2pdma.h>
 #include <linux/scatterlist.h>
 
@@ -15,6 +16,7 @@
 
 #include "nvmet.h"
 
+struct nvmet_poll_data *t;
 struct workqueue_struct *buffered_io_wq;
 static const struct nvmet_fabrics_ops *nvmet_transports[NVMF_TRTYPE_MAX];
 static DEFINE_IDA(cntlid_ida);
@@ -1455,6 +1457,16 @@ void nvmet_subsys_put(struct nvmet_subsys *subsys)
        kref_put(&subsys->ref, nvmet_subsys_free);
 }
 
+static int nvmet_start_polling(void)
+{
+         return nvmet_bdev_start_polling();
+}
+
+static void nvmet_stop_polling(void)
+{
+       nvmet_bdev_stop_polling();
+}
+
 static int __init nvmet_init(void)
 {
        int error;
@@ -1475,8 +1487,15 @@ static int __init nvmet_init(void)
        error = nvmet_init_configfs();
        if (error)
                goto out_exit_discovery;
+
+       error = nvmet_start_polling();
+       if (error)
+               goto out_exit_poll;
+
        return 0;
 
+out_exit_poll:
+       nvmet_bdev_stop_polling();
 out_exit_discovery:
        nvmet_exit_discovery();
 out_free_work_queue:
@@ -1487,6 +1506,7 @@ static int __init nvmet_init(void)
 
 static void __exit nvmet_exit(void)
 {
+       nvmet_stop_polling();
        nvmet_exit_configfs();
        nvmet_exit_discovery();
        ida_destroy(&cntlid_ida);
diff --git a/drivers/nvme/target/io-cmd-bdev.c b/drivers/nvme/target/io-cmd-bdev.c
index f2618dc2ef3a..6e69f669653f 100644
--- a/drivers/nvme/target/io-cmd-bdev.c
+++ b/drivers/nvme/target/io-cmd-bdev.c
@@ -6,8 +6,81 @@
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/blkdev.h>
 #include <linux/module.h>
+#include <linux/sched/signal.h>
+#include <linux/kthread.h>
+
 #include "nvmet.h"
 
+static inline enum nvmet_poll_thread_state
+nvmet_poll_thread_get_state(struct nvmet_poll_data *t)
+{
+       enum nvmet_poll_thread_state state;
+
+       mutex_lock(&t->list_mutex);
+       state = t->state;
+       mutex_unlock(&t->list_mutex);
+
+       return state;
+}
+
+static inline enum nvmet_poll_thread_state
+nvmet_poll_thread_set_state(struct nvmet_poll_data *t,
+                           enum nvmet_poll_thread_state state)
+{
+       mutex_lock(&t->list_mutex);
+       /* XXX: validate right state here */
+       t->state = state;
+       mutex_unlock(&t->list_mutex);
+
+       return state;
+}
+
+void nvmet_bdev_stop_polling(void)
+{
+       int i;
+
+       for (i = 0; i < num_online_cpus(); i++) {
+               if (!t[i].thread)
+                       continue;
+               kthread_park(t[i].thread);
+               kthread_stop(t[i].thread);
+               nvmet_poll_thread_set_state(t, NVMET_POLL_THREAD_EXITED);
+       }
+}
+
+int nvmet_bdev_start_polling(void)
+{
+       int ret = 0;
+       int i;
+
+       t = kzalloc(sizeof(*t) * num_online_cpus(), GFP_KERNEL);
+       if (!t) {
+               ret = -ENOMEM;
+               goto out;
+       }
+
+       for (i = 0; i < num_online_cpus(); i++) {
+               init_waitqueue_head(&t[i].poll_waitq);
+               init_completion(&t[i].thread_started);
+               t[i].thread = kthread_create(nvmet_poll_thread, &t[i],
+                                                 "nvmet_poll_thread/%d", i);
+               if (IS_ERR(t[i].thread)) {
+                       ret = PTR_ERR(t[i].thread);
+                       goto out;
+               }
+               kthread_bind(t[i].thread, i);
+       }
+
+       nvmet_poll_thread_set_state(t, NVMET_POLL_THREAD_READY);
+
+       for (i = 0; i < num_online_cpus(); i++) {
+               wake_up_process(t[i].thread);
+               wait_for_completion(&t[i].thread_started);
+       }
+out:
+       return ret;
+}
+
 void nvmet_bdev_set_limits(struct block_device *bdev, struct nvme_id_ns *id)
 {
        const struct queue_limits *ql = &bdev_get_queue(bdev)->limits;
@@ -133,15 +206,137 @@ static u16 blk_to_nvme_status(struct nvmet_req *req, blk_status_t blk_sts)
        return status;
 }
 
-static void nvmet_bio_done(struct bio *bio)
+static void nvmet_req_cmd_print(struct nvmet_req *req, char const *str, int line)
 {
-       struct nvmet_req *req = bio->bi_private;
+       switch (req->cmd->common.opcode) {
+       case nvme_cmd_read:
+               pr_info("%30s %4d %5s %p\n", str, line, "read ", req);
+               break;
+       case nvme_cmd_write:
+               pr_info("%30s %4d %5s %p\n", str, line, "write", req);
+               break;
+       case nvme_cmd_flush:
+               pr_info("%30s %4d %5s %p\n", str, line, "flush", req);
+               break;
+       case nvme_cmd_dsm:
+               pr_info("%30s %4d %5s %p\n", str, line, "dsm", req);
+               break;
+       case nvme_cmd_write_zeroes:
+               pr_info("%30s %4d %5s %p\n", str, line, "wz", req);
+               break;
+       default:
+               pr_info("%30s %s %d UKNOWN\n", str, __func__, __LINE__);
+       }
+}
 
+static void nvmet_bdev_req_complete(struct nvmet_req *req)
+{
+       struct bio *bio = req->b.last_bio;
+
+       nvmet_req_cmd_print(req, __func__, __LINE__);
        nvmet_req_complete(req, blk_to_nvme_status(req, bio->bi_status));
        if (bio != &req->b.inline_bio)
                bio_put(bio);
 }
 
+static void nvmet_bio_done(struct bio *bio)
+{
+       struct nvmet_req *req = bio->bi_private;
+
+       nvmet_req_cmd_print(req, __func__, __LINE__);
+       req->b.last_bio = bio;
+       req->b.poll ? complete(&req->b.wait) : nvmet_bdev_req_complete(req);
+}
+
+static inline bool nvmet_poll_thread_sleep(struct nvmet_poll_data *t)
+{
+       DEFINE_WAIT(wait);
+
+       prepare_to_wait(&t->poll_waitq, &wait, TASK_INTERRUPTIBLE);
+       smp_mb();
+       if (kthread_should_park()) {
+               finish_wait(&t->poll_waitq, &wait);
+               return false;
+       }
+       if (signal_pending(current))
+               flush_signals(current);
+       schedule();
+       finish_wait(&t->poll_waitq, &wait);
+       return true;
+}
+
+static inline void nvmet_bdev_req_poll_complete(struct nvmet_req *req)
+{
+       struct request_queue *q = bdev_get_queue(req->ns->bdev);
+
+       nvmet_req_cmd_print(req, __func__, __LINE__);
+       while (req->b.poll && !completion_done(&req->b.wait)) {
+               blk_poll(q, req->b.cookie, true);
+               io_schedule();
+       }
+       nvmet_bdev_req_complete(req);
+}
+
+static struct nvmet_req *nvmet_bdev_get_poll_req(struct nvmet_poll_data *t)
+{
+       struct nvmet_req *req = NULL;
+
+       mutex_lock(&t->list_mutex);
+       req = list_first_entry_or_null(&t->list, struct nvmet_req,
+                                      poll_entry);
+       if (req)
+               list_del(&req->poll_entry);
+       mutex_unlock(&t->list_mutex);
+
+       return req;
+}
+
+static inline void nvmet_bdev_process_poll_list(struct nvmet_poll_data *t)
+{
+       if (list_empty(&t->list))
+               return;
+
+       /*
+        * All the submitted requests are present on the poll list, drain the
+        * poll list by completing the requests one by one.
+        */
+       do {
+               struct nvmet_req *req = nvmet_bdev_get_poll_req(t);
+               if (!req)
+                       break;
+
+               WARN_ON_ONCE(req->b.poll == false);
+
+               nvmet_bdev_req_poll_complete(req);
+       } while (1);
+}
+
+int nvmet_poll_thread(void *data)
+{
+       struct nvmet_poll_data *t = (struct nvmet_poll_data *) data;
+
+       nvmet_poll_thread_set_state(t, NVMET_POLL_THREAD_RUNNING);
+       mutex_init(&t->list_mutex);
+       INIT_LIST_HEAD(&t->list);
+       complete(&t->thread_started);
+
+       while (!kthread_should_park()) {
+               nvmet_bdev_process_poll_list(t);
+               /*
+                * When poll_list is empty just go to sleep, after wake up
+                * restart again.
+                */
+               if (!nvmet_poll_thread_sleep(t))
+                       break;
+       }
+       nvmet_poll_thread_set_state(t, NVMET_POLL_THREAD_STOPPED);
+       /* finish any remaining requests. */
+       nvmet_bdev_process_poll_list(t);
+
+       kthread_parkme();
+       return 0;
+}
+
 static void nvmet_bdev_execute_rw(struct nvmet_req *req)
 {
        int sg_cnt = req->sg_cnt;
@@ -149,6 +344,7 @@ static void nvmet_bdev_execute_rw(struct nvmet_req *req)
        struct scatterlist *sg;
        sector_t sector;
        int op, op_flags = 0, i;
+       unsigned int tid;
 
        if (!req->sg_cnt) {
                nvmet_req_complete(req, 0);
@@ -200,7 +396,31 @@ static void nvmet_bdev_execute_rw(struct nvmet_req *req)
                sg_cnt--;
        }
 
-       submit_bio(bio);
+       /* XXX: find a right way to get the thread id from the processor
+        * on which we are running right now.
+        */
+       tid = get_cpu();
+       put_cpu();
+       nvmet_req_cmd_print(req, __func__, __LINE__);
+       if (nvmet_poll_thread_get_state(&t[tid]) != NVMET_POLL_THREAD_RUNNING)
+               req->b.poll = false;
+       else
+               init_completion(&req->b.wait);
+
+       req->b.cookie = submit_bio(bio);
+
+       if (req->b.poll) {
+               /*
+                * Add this I/O to poll_list for this thread and complete in
+                * the poll thread context then wake up the poll thread.
+                */
+               mutex_lock(&t[tid].list_mutex);
+               list_add_tail(&req->poll_entry, &t[tid].list);
+               mutex_unlock(&t[tid].list_mutex);
+
+               if (waitqueue_active(&t[tid].poll_waitq))
+                       wake_up(&t[tid].poll_waitq);
+       }
 }
 
 static void nvmet_bdev_execute_flush(struct nvmet_req *req)
@@ -213,6 +433,7 @@ static void nvmet_bdev_execute_flush(struct nvmet_req *req)
        bio->bi_end_io = nvmet_bio_done;
        bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
 
+       nvmet_req_cmd_print(req, __func__, __LINE__);
        submit_bio(bio);
 }
 
@@ -234,9 +455,11 @@ static u16 nvmet_bdev_discard_range(struct nvmet_req *req,
                        le32_to_cpu(range->nlb) << (ns->blksize_shift - 9),
                        GFP_KERNEL, 0, bio);
        if (ret && ret != -EOPNOTSUPP) {
+               pr_info("%s %d\n", __func__, __LINE__);
                req->error_slba = le64_to_cpu(range->slba);
                return errno_to_nvme_status(req, ret);
        }
+       pr_info("%s %d\n", __func__, __LINE__);
        return NVME_SC_SUCCESS;
 }
 
@@ -263,9 +486,12 @@ static void nvmet_bdev_execute_discard(struct nvmet_req *req)
                bio->bi_end_io = nvmet_bio_done;
                if (status)
                        bio_io_error(bio);
-               else
+               else {
+                       nvmet_req_cmd_print(req, __func__, __LINE__);
                        submit_bio(bio);
+               }
        } else {
+               nvmet_req_cmd_print(req, __func__, __LINE__);
                nvmet_req_complete(req, status);
        }
 }
@@ -303,8 +529,10 @@ static void nvmet_bdev_execute_write_zeroes(struct nvmet_req *req)
        if (bio) {
                bio->bi_private = req;
                bio->bi_end_io = nvmet_bio_done;
+               nvmet_req_cmd_print(req, __func__, __LINE__);
                submit_bio(bio);
        } else {
+               nvmet_req_cmd_print(req, __func__, __LINE__);
                nvmet_req_complete(req, errno_to_nvme_status(req, ret));
        }
 }
@@ -316,19 +544,23 @@ u16 nvmet_bdev_parse_io_cmd(struct nvmet_req *req)
        switch (cmd->common.opcode) {
        case nvme_cmd_read:
        case nvme_cmd_write:
+               req->b.poll = true;
                req->execute = nvmet_bdev_execute_rw;
                req->data_len = nvmet_rw_len(req);
                return 0;
        case nvme_cmd_flush:
+               req->b.poll = false;
                req->execute = nvmet_bdev_execute_flush;
                req->data_len = 0;
                return 0;
        case nvme_cmd_dsm:
+               req->b.poll = false;
                req->execute = nvmet_bdev_execute_dsm;
                req->data_len = (le32_to_cpu(cmd->dsm.nr) + 1) *
                        sizeof(struct nvme_dsm_range);
                return 0;
        case nvme_cmd_write_zeroes:
+               req->b.poll = false;
                req->execute = nvmet_bdev_execute_write_zeroes;
                req->data_len = 0;
                return 0;
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index c51f8dd01dc4..10105535ea83 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -49,6 +49,22 @@
 #define IPO_IATTR_CONNECT_SQE(x)       \
        (cpu_to_le32(offsetof(struct nvmf_connect_command, x)))
 
+enum nvmet_poll_thread_state {
+       NVMET_POLL_THREAD_READY,
+       NVMET_POLL_THREAD_RUNNING,
+       NVMET_POLL_THREAD_STOPPED,
+       NVMET_POLL_THREAD_EXITED,
+};
+
+struct nvmet_poll_data {
+       struct completion               thread_started;
+       struct task_struct              *thread;
+       struct list_head                list;
+       struct mutex                    list_mutex;
+       wait_queue_head_t               poll_waitq;
+       enum nvmet_poll_thread_state    state;
+};
+
 struct nvmet_ns {
        struct list_head        dev_link;
        struct percpu_ref       ref;
@@ -295,6 +311,10 @@ struct nvmet_req {
        union {
                struct {
                        struct bio      inline_bio;
+                       blk_qc_t                cookie;
+                       struct completion       wait;
+                       struct bio              *last_bio;
+                       bool                    poll;
                } b;
                struct {
                        bool                    mpool_alloc;
@@ -318,8 +338,10 @@ struct nvmet_req {
        struct device           *p2p_client;
        u16                     error_loc;
        u64                     error_slba;
+       struct list_head        poll_entry;
 };
 
+extern struct nvmet_poll_data *t;
 extern struct workqueue_struct *buffered_io_wq;
 
 static inline void nvmet_set_result(struct nvmet_req *req, u32 result)
@@ -444,6 +466,10 @@ void nvmet_subsys_disc_changed(struct nvmet_subsys *subsys,
 void nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,
                u8 event_info, u8 log_page);
 
+int nvmet_bdev_start_polling(void);
+void nvmet_bdev_stop_polling(void);
+int nvmet_poll_thread(void *data);

