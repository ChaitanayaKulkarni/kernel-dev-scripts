From b6f08026995fe21b8a271107b71935e0ced2fb1a Mon Sep 17 00:00:00 2001
From: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date: Fri, 7 Aug 2020 21:12:58 -0700
Subject: [PATCH 11/11] nvmet: use minimized version of blk_rq_append_bio

The function blk_rq_append_bio() is a genereric API written for
all types driver (having bounce buffers) and different context
(where request already having bio rq->bio != NULL). With NVMe passthru
request based driver we allocate fresh request each time we call
blk_rq_append_bio() so rq->bio will always be NULL.

Just before the call to the blk_rq_append_bio() we already iterate
over sg_cnt which should be == nr_segs. blk_rq_append_bio() again does
this calculation which is a duplication of the work.

The NVMe PCIe driver does not use the queue bounce mechanism. In order
to find this out for each request processing in the passthru
blk_rq_append_bio() does extra work by iterating over each segment to
find out bounce is not needed.

So for passthru driver recalculating the segments, bounce check and
ll_back_merge_code is never needed such that we can get away with the
minimal version of the blk_rq_append_bio() which removes the error
check in the fast path along with the extra variable.

This patch updates the nvme_passthru_map_sg() such that it does minimal
block request layer request bio append in context of NVMe Passthru
driver. Follwing are perf numbers :-

With current implementation (blk_rq_append_bio()) :-
+    5.80%     0.02%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    5.44%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    4.88%     0.00%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    5.44%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    4.86%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    5.17%     0.00%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd

With this patch :-
+    3.14%     0.02%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    3.26%     0.01%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    5.37%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    5.18%     0.02%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    4.84%     0.02%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    4.87%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
---
Perf number for nvmet_passthru_execute_cmd()

With current implementation (blk_rq_append_bio()) top 5 picks for each round :-
+    5.80%     0.02%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.88%     0.02%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.46%     0.02%  kworker/7:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.37%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.34%     0.02%  kworker/10:1-ev  [nvmet]  [k] nvmet_passthru_execute_cmd

+    5.44%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.81%     0.01%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.36%     0.02%  kworker/2:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.34%     0.02%  kworker/7:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.30%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    4.88%     0.00%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.62%     0.02%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.41%     0.02%  kworker/3:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.31%     0.02%  kworker/2:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.30%     0.02%  kworker/4:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd

+    5.44%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.94%     0.02%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.32%     0.02%  kworker/6:1-xfs  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.31%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.26%     0.02%  kworker/8:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    4.86%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.59%     0.01%  kworker/1:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.38%     0.02%  kworker/6:1-xfs  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.33%     0.02%  kworker/5:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.29%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    5.17%     0.00%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.69%     0.01%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.30%     0.02%  kworker/3:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.29%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.27%     0.02%  kworker/6:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd


With this patch with this patch top 5 pics for each round :-
+    3.14%     0.02%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.25%     0.04%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.90%     0.04%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.88%     0.05%  kworker/3:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.83%     0.04%  kworker/2:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd


+    3.26%     0.01%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.54%     0.04%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.84%     0.04%  kworker/5:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.83%     0.04%  kworker/3:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.83%     0.04%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    5.37%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.83%     0.02%  kworker/1:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.42%     0.02%  kworker/4:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.39%     0.02%  kworker/7:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.39%     0.02%  kworker/3:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    5.18%     0.02%  kworker/0:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.51%     0.02%  kworker/1:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.33%     0.03%  kworker/2:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.30%     0.03%  kworker/6:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.29%     0.02%  kworker/3:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd

+    4.84%     0.02%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.77%     0.02%  kworker/1:2-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.37%     0.03%  kworker/10:1-mm  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.32%     0.02%  kworker/2:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.32%     0.02%  kworker/8:3-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd

+    4.87%     0.01%  kworker/0:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    2.58%     0.02%  kworker/1:2-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.40%     0.02%  kworker/6:1-mm_  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.37%     0.02%  kworker/2:1-eve  [nvmet]  [k] nvmet_passthru_execute_cmd
+    1.35%     0.02%  kworker/12:1-ev  [nvmet]  [k] nvmet_passthru_execute_cmd
---
 drivers/nvme/target/passthru.c | 9 +++------
 1 file changed, 3 insertions(+), 6 deletions(-)

diff --git a/drivers/nvme/target/passthru.c b/drivers/nvme/target/passthru.c
index 4288f1418b5a..b26fdba9de03 100644
--- a/drivers/nvme/target/passthru.c
+++ b/drivers/nvme/target/passthru.c
@@ -9,6 +9,7 @@
  */
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/module.h>
+#include <linux/blk-mq.h>
 
 #include "../host/nvme.h"
 #include "nvmet.h"
@@ -207,7 +208,7 @@ static int nvmet_passthru_map_sg(struct nvmet_req *req, struct request *rq)
 	int sg_cnt = req->sg_cnt;
 	struct scatterlist *sg;
 	struct bio *bio;
-	int i, ret;
+	int i;
 
 	if (nvme_is_write(req->cmd))
 		op_flags = REQ_SYNC | REQ_IDLE;
@@ -229,11 +230,7 @@ static int nvmet_passthru_map_sg(struct nvmet_req *req, struct request *rq)
 		sg_cnt--;
 	}
 
-	ret = blk_rq_append_bio(rq, &bio);
-	if (unlikely(ret)) {
-		bio_put(bio);
-		return ret;
-	}
+	blk_rq_bio_prep(rq, bio, req->sg_cnt);
 
 	return 0;
 }
-- 
2.22.1

