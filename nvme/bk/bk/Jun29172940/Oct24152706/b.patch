

This patch implements polling for the file-backed NVMeOF target
namespace.

In the I/O submission code path, for each I/O request we queue the
polling work (nvmet_file_io_poll_work()) after submitting the kiocb and
return from the submission thread.

The new function nvmet_file_io_poll_work() runs in a tight loop calling
blk_poll() indirectly from the iocb's file operations->iopoll() followed
by the io_schedule().

In the I/O completion path, completion callback signals the polling work
routine about the completion of this I/O so that it can exit the loop.

By default we only enable polling for the READ/WRITE I/Os when we don't
set IOCB_NOWAIT and for direct I/O where we expect the async I/O
completion.

Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
---
 drivers/nvme/target/io-cmd-file.c | 74 +++++++++++++++++++++++++++----
 1 file changed, 66 insertions(+), 8 deletions(-)

diff --git a/drivers/nvme/target/io-cmd-file.c b/drivers/nvme/target/io-cmd-file.c
index 7171830cffbf..495fe3f5a55e 100644
--- a/drivers/nvme/target/io-cmd-file.c
+++ b/drivers/nvme/target/io-cmd-file.c
@@ -8,6 +8,8 @@
 #include <linux/uio.h>
 #include <linux/falloc.h>
 #include <linux/file.h>
+#include <linux/delay.h>
+
 #include "nvmet.h"
 
 #define NVMET_MAX_MPOOL_BVEC		16
@@ -116,9 +118,8 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	return call_iter(iocb, &iter);
 }
 
-static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
+static void nvmet_file_req_complete(struct nvmet_req *req)
 {
-	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
 	u16 status = NVME_SC_SUCCESS;
 
 	if (req->f.bvec != req->inline_bvec) {
@@ -128,13 +129,45 @@ static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 			mempool_free(req->f.bvec, req->ns->bvec_pool);
 	}
 
-	if (unlikely(ret != req->data_len))
-		status = errno_to_nvme_status(req, ret);
+	if (unlikely(req->f.iosize != req->data_len))
+		status = errno_to_nvme_status(req, req->f.iosize);
 	nvmet_req_complete(req, status);
 }
 
+static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
+{
+	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
+
+	req->f.iosize = ret;
+
+	if (req->f.polled) {
+		complete(&req->f.waiting);
+		return;
+	}
+
+	nvmet_file_req_complete(req);
+}
+
+static void nvmet_file_io_poll_work(struct work_struct *w)
+{
+	struct nvmet_req *req = container_of(w, struct nvmet_req, f.work);
+
+	while (req->f.polled && !completion_done(&req->f.waiting)) {
+		req->f.iocb.ki_filp->f_op->iopoll(&req->f.iocb, true);
+		io_schedule();
+		/*
+		 * Without following msleep nvmet_file_io_done() doen't get
+		 * schedule resulting this loop to runing into forever when
+		 * tested in qemu and nvme-loop.
+		 */
+		msleep(1);
+	}
+	nvmet_file_req_complete(req);
+}
+
 static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 {
+	struct kiocb *iocb = &req->f.iocb;
 	ssize_t nr_bvec = req->sg_cnt;
 	unsigned long bv_cnt = 0;
 	bool is_sync = false;
@@ -153,7 +186,7 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 		return true;
 	}
 
-	memset(&req->f.iocb, 0, sizeof(struct kiocb));
+	memset(iocb, 0, sizeof(struct kiocb));
 	for_each_sg(req->sg, sg, req->sg_cnt, i) {
 		nvmet_file_init_bvec(&req->f.bvec[bv_cnt], sg);
 		len += req->f.bvec[bv_cnt].bv_len;
@@ -189,13 +222,29 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 	 * A NULL ki_complete ask for synchronous execution, which we want
 	 * for the IOCB_NOWAIT case.
 	 */
-	if (!(ki_flags & IOCB_NOWAIT))
-		req->f.iocb.ki_complete = nvmet_file_io_done;
+	if (!(ki_flags & IOCB_NOWAIT)) {
+		iocb->ki_complete = nvmet_file_io_done;
+		if (req->ns->file->f_op->iopoll) {
+			/*
+			 * This is to avoid race between poll work and I/O
+			 * completion when req->f.waiting is accessed in the
+			 * nvmet_file_io_done() and nvmet_file_io_poll_work().
+			 * We can only determine if this I/O is polled or not
+			 * based on return value of nvmet_file_submit_bvec().
+			 * (i.e. -EIOCBQUEUED)
+			 */
+			req->f.polled = true;
+			init_completion(&req->f.waiting);
+			INIT_WORK(&req->f.work, nvmet_file_io_poll_work);
+		}
+	}
 
 	ret = nvmet_file_submit_bvec(req, pos, bv_cnt, total_len, ki_flags);
 
 	switch (ret) {
 	case -EIOCBQUEUED:
+		if (req->f.polled)
+			queue_work(io_poll_wq, &req->f.work);
 		return true;
 	case -EAGAIN:
 		if (WARN_ON_ONCE(!(ki_flags & IOCB_NOWAIT)))
@@ -213,7 +262,12 @@ static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 	}
 
 complete:
-	nvmet_file_io_done(&req->f.iocb, ret, 0);
+	/*
+	 * If we are here, then I/O is synchronously completed and ret holds
+	 * number of bytes transfered.
+	 */
+	req->f.iosize = ret;
+	nvmet_file_req_complete(req);
 	return true;
 }
 
@@ -372,19 +426,23 @@ u16 nvmet_file_parse_io_cmd(struct nvmet_req *req)
 	switch (cmd->common.opcode) {
 	case nvme_cmd_read:
 	case nvme_cmd_write:
+		req->f.polled = false;
 		req->execute = nvmet_file_execute_rw;
 		req->data_len = nvmet_rw_len(req);
 		return 0;
 	case nvme_cmd_flush:
+		req->f.polled = false;
 		req->execute = nvmet_file_execute_flush;
 		req->data_len = 0;
 		return 0;
 	case nvme_cmd_dsm:
+		req->f.polled = false;
 		req->execute = nvmet_file_execute_dsm;
 		req->data_len = (le32_to_cpu(cmd->dsm.nr) + 1) *
 			sizeof(struct nvme_dsm_range);
 		return 0;
 	case nvme_cmd_write_zeroes:
+		req->f.polled = false;
 		req->execute = nvmet_file_execute_write_zeroes;
 		req->data_len = 0;
 		return 0;
-- 
2.22.1



