From 7aba725d81dbd6a42f997cd6f6ec8f06666a96be Mon Sep 17 00:00:00 2001
From: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date: Sat, 19 Oct 2019 01:10:04 -0700
Subject: [PATCH] nvmet: add bdev-ns polling support

Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
---
 drivers/nvme/target/core.c        |  10 ++
 drivers/nvme/target/io-cmd-bdev.c | 154 +++++++++++++++++++++++++++++-
 drivers/nvme/target/nvmet.h       |  18 ++++
 3 files changed, 179 insertions(+), 3 deletions(-)

diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 6b39cfc6ade1..3bb4db302763 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -7,6 +7,7 @@
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/rculist.h>
+#include <linux/kthread.h>
 #include <linux/pci-p2pdma.h>
 #include <linux/scatterlist.h>
 
@@ -15,6 +16,7 @@
 
 #include "nvmet.h"
 
+struct nvmet_poll_data *t;
 struct workqueue_struct *buffered_io_wq;
 static const struct nvmet_fabrics_ops *nvmet_transports[NVMF_TRTYPE_MAX];
 static DEFINE_IDA(cntlid_ida);
@@ -1475,8 +1477,15 @@ static int __init nvmet_init(void)
 	error = nvmet_init_configfs();
 	if (error)
 		goto out_exit_discovery;
+
+	error = nvmet_bdev_start_polling();
+	if (error)
+		goto out_exit_poll;
+
 	return 0;
 
+out_exit_poll:
+	nvmet_bdev_stop_polling();
 out_exit_discovery:
 	nvmet_exit_discovery();
 out_free_work_queue:
@@ -1487,6 +1496,7 @@ static int __init nvmet_init(void)
 
 static void __exit nvmet_exit(void)
 {
+	nvmet_bdev_stop_polling();
 	nvmet_exit_configfs();
 	nvmet_exit_discovery();
 	ida_destroy(&cntlid_ida);
diff --git a/drivers/nvme/target/io-cmd-bdev.c b/drivers/nvme/target/io-cmd-bdev.c
index f2618dc2ef3a..1e2f4fd140d8 100644
--- a/drivers/nvme/target/io-cmd-bdev.c
+++ b/drivers/nvme/target/io-cmd-bdev.c
@@ -6,8 +6,54 @@
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/blkdev.h>
 #include <linux/module.h>
+#include <linux/sched/signal.h>
+#include <linux/kthread.h>
+
 #include "nvmet.h"
 
+void nvmet_bdev_stop_polling(void)
+{
+	int i;
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		if (t[i].poll_thread) {
+			kthread_park(t[i].poll_thread);
+			kthread_stop(t[i].poll_thread);
+		}
+	}
+}
+
+int nvmet_bdev_start_polling(void)
+{
+	int ret = 0;
+	int i;
+
+	t = kzalloc(sizeof(*t) * num_online_cpus(), GFP_KERNEL);
+	if (!t) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		init_waitqueue_head(&t[i].poll_waitq);
+		init_completion(&t[i].poll_thread_started);
+		t[i].poll_thread = kthread_create(nvmet_poll_thread,
+					&t[i], "nvmet_poll_thread/%d", i);
+		if (IS_ERR(t[i].poll_thread)) {
+			ret = PTR_ERR(t[i].poll_thread);
+			goto out;
+		}
+		kthread_bind(t[i].poll_thread, i);
+	}
+
+	for (i = 0; i < num_online_cpus(); i++) {
+		wake_up_process(t[i].poll_thread);
+		wait_for_completion(&t[i].poll_thread_started);
+	}
+out:
+	return ret;
+}
+
 void nvmet_bdev_set_limits(struct block_device *bdev, struct nvme_id_ns *id)
 {
 	const struct queue_limits *ql = &bdev_get_queue(bdev)->limits;
@@ -133,15 +179,105 @@ static u16 blk_to_nvme_status(struct nvmet_req *req, blk_status_t blk_sts)
 	return status;
 }
 
-static void nvmet_bio_done(struct bio *bio)
+static void nvmet_bdev_req_complete(struct nvmet_req *req)
 {
-	struct nvmet_req *req = bio->bi_private;
+	struct bio *bio = req->b.last_bio;
 
 	nvmet_req_complete(req, blk_to_nvme_status(req, bio->bi_status));
 	if (bio != &req->b.inline_bio)
 		bio_put(bio);
 }
 
+static inline void nvmet_bdev_req_poll_complete(struct nvmet_req *req)
+{
+	struct request_queue *q = bdev_get_queue(req->ns->bdev);
+
+	while (!completion_done(&req->b.waiting)) {
+		blk_poll(q, req->b.cookie, true);
+		io_schedule();
+	}
+
+	nvmet_bdev_req_complete(req);
+}
+
+static void nvmet_bio_done(struct bio *bio)
+{
+	struct nvmet_req *req = bio->bi_private;
+
+	req->b.last_bio = bio;
+	if (req->b.polled) {
+		complete(&req->b.waiting);
+		return;
+	}
+	nvmet_bdev_req_complete(req);
+}
+
+static inline bool nvmet_poll_thread_sleep(struct nvmet_poll_data *t)
+{
+	DEFINE_WAIT(wait);
+
+	prepare_to_wait(&t->poll_waitq, &wait, TASK_INTERRUPTIBLE);
+	smp_mb();
+	if (kthread_should_park()) {
+		finish_wait(&t->poll_waitq, &wait);
+		return false;
+	}
+	if (signal_pending(current))
+		flush_signals(current);
+	schedule();
+	finish_wait(&t->poll_waitq, &wait);
+	return true;
+}
+
+static inline struct nvmet_req *nvmet_bdev_get_poll_req(struct nvmet_poll_data *t)
+{
+	struct nvmet_req *req = NULL;
+
+	mutex_lock(&t->poll_list_mutex);
+	req = list_first_entry_or_null(&t->poll_list, struct nvmet_req,
+				       poll_entry);
+	if (req)
+		list_del(&req->poll_entry);
+	mutex_unlock(&t->poll_list_mutex);
+
+	return req;
+}
+
+int nvmet_poll_thread(void *data)
+{
+	struct nvmet_poll_data *t = (struct nvmet_poll_data *) data;
+	struct nvmet_req *req;
+
+	INIT_LIST_HEAD(&t->poll_list);
+	mutex_init(&t->poll_list_mutex);
+
+	complete(&t->poll_thread_started);
+	while (!kthread_should_park()) {
+		/*
+		 * When poll_list is empty just fo to sleep, after wake up
+		 * restart again.
+		 */
+		if (list_empty(&t->poll_list)) {
+			if (!nvmet_poll_thread_sleep(t))
+				break;
+			continue;
+		}
+		while (1) {
+			/*
+			 * All the submitted requests are present on the
+			 * poll list, drain the poll list by completing the
+			 * requests one by one.
+			 */
+			req = nvmet_bdev_get_poll_req(t);
+			if (!req)
+				break;
+			nvmet_bdev_req_poll_complete(req);
+		}
+	}
+	kthread_parkme();
+	return 0;
+}
+
 static void nvmet_bdev_execute_rw(struct nvmet_req *req)
 {
 	int sg_cnt = req->sg_cnt;
@@ -200,7 +336,15 @@ static void nvmet_bdev_execute_rw(struct nvmet_req *req)
 		sg_cnt--;
 	}
 
-	submit_bio(bio);
+	init_completion(&req->b.waiting);
+	req->b.cookie = submit_bio(bio);
+
+	mutex_lock(&t[smp_processor_id()].poll_list_mutex);
+	list_add_tail(&req->poll_entry, &t[smp_processor_id()].poll_list);
+	mutex_unlock(&t[smp_processor_id()].poll_list_mutex);
+
+	if (waitqueue_active(&t[smp_processor_id()].poll_waitq))
+		wake_up(&t[smp_processor_id()].poll_waitq);
 }
 
 static void nvmet_bdev_execute_flush(struct nvmet_req *req)
@@ -316,19 +460,23 @@ u16 nvmet_bdev_parse_io_cmd(struct nvmet_req *req)
 	switch (cmd->common.opcode) {
 	case nvme_cmd_read:
 	case nvme_cmd_write:
+		req->b.polled = true;
 		req->execute = nvmet_bdev_execute_rw;
 		req->data_len = nvmet_rw_len(req);
 		return 0;
 	case nvme_cmd_flush:
+		req->b.polled = false;
 		req->execute = nvmet_bdev_execute_flush;
 		req->data_len = 0;
 		return 0;
 	case nvme_cmd_dsm:
+		req->b.polled = false;
 		req->execute = nvmet_bdev_execute_dsm;
 		req->data_len = (le32_to_cpu(cmd->dsm.nr) + 1) *
 			sizeof(struct nvme_dsm_range);
 		return 0;
 	case nvme_cmd_write_zeroes:
+		req->b.polled = false;
 		req->execute = nvmet_bdev_execute_write_zeroes;
 		req->data_len = 0;
 		return 0;
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index c51f8dd01dc4..399828651186 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -49,6 +49,14 @@
 #define IPO_IATTR_CONNECT_SQE(x)	\
 	(cpu_to_le32(offsetof(struct nvmf_connect_command, x)))
 
+struct nvmet_poll_data {
+	struct completion poll_thread_started;
+	struct task_struct *poll_thread;
+	struct list_head poll_list;
+	struct mutex poll_list_mutex;
+	wait_queue_head_t poll_waitq;
+};
+
 struct nvmet_ns {
 	struct list_head	dev_link;
 	struct percpu_ref	ref;
@@ -295,6 +303,10 @@ struct nvmet_req {
 	union {
 		struct {
 			struct bio      inline_bio;
+			blk_qc_t		cookie;
+			struct completion	waiting;
+			struct bio     		*last_bio;
+			bool			polled;
 		} b;
 		struct {
 			bool			mpool_alloc;
@@ -318,8 +330,10 @@ struct nvmet_req {
 	struct device		*p2p_client;
 	u16			error_loc;
 	u64			error_slba;
+	struct list_head	poll_entry;
 };
 
+extern struct nvmet_poll_data *t;
 extern struct workqueue_struct *buffered_io_wq;
 
 static inline void nvmet_set_result(struct nvmet_req *req, u32 result)
@@ -444,6 +458,10 @@ void nvmet_subsys_disc_changed(struct nvmet_subsys *subsys,
 void nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,
 		u8 event_info, u8 log_page);
 
+int nvmet_bdev_start_polling(void);
+void nvmet_bdev_stop_polling(void);
+int nvmet_poll_thread(void *data);
+
 #define NVMET_QUEUE_SIZE	1024
 #define NVMET_NR_QUEUES		128
 #define NVMET_MAX_CMD		NVMET_QUEUE_SIZE
-- 
2.22.1

