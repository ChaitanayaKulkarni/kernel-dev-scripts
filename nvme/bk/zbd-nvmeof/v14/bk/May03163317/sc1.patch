From 34127e9c2a48d6de998d90920735856d4c9e82a3 Mon Sep 17 00:00:00 2001

From: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
Date: Mon, 15 Jul 2019 13:17:53 -0700
Subject: [RFC PATCH 1/4] block : add REQ_OP_SIMPLE_COPY support

This patch adds block layer plumbing for the REQ_OP_SIMPLE_COPY command
which is required for the NVMe Simple Copy command implementation. This
NVMe command is aimed at the providing a way to do garbage collection
on zoned media.

We add following block layer support for simple copy :-
1. Request merging
2. Various Macros, queue limits and queue flags.
3. Sysfs interface.
4. IOCTL and Kernel API.

* TODO: Overall code cleanup.
* TODO: Add blktests testcases for code coverage.
* TODO: Create a user level API for accepting multiple ranges.
* TODO: Write a testcase to trigger merge code for simple copy.
* TODO: Add CONFIG_BLK_DEV_SIMPLE_COPY KConfig option.
* TODO: Test corner cases and various limits.

Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
---
 block/blk-core.c          |  37 ++++++++++
 block/blk-lib.c           | 149 ++++++++++++++++++++++++++++++++++++++
 block/blk-merge.c         |  41 +++++++++++
 block/blk-mq-sched.c      |   5 ++
 block/blk-settings.c      |  35 +++++++++
 block/blk-sysfs.c         |  13 ++++
 block/blk.h               |   2 +
 block/ioctl.c             |   2 +
 include/linux/bio.h       |  11 ++-
 include/linux/blk_types.h |  13 ++++
 include/linux/blkdev.h    |  47 +++++++++++-
 include/linux/elevator.h  |   1 +
 include/uapi/linux/fs.h   |  14 ++++
 13 files changed, 365 insertions(+), 5 deletions(-)

diff --git a/block/blk-core.c b/block/blk-core.c
index d5e668ec751b..bf5a9846e64c 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -667,6 +667,36 @@ bool bio_attempt_discard_merge(struct request_queue *q, struct request *req,
 	return false;
 }
 
+bool bio_attempt_simple_copy_merge(struct request_queue *q, struct request *req,
+		struct bio *bio)
+{
+	unsigned short segments = blk_rq_nr_simple_copy_segments(req);
+
+	if (segments >= queue_max_simple_copy_segments(q))
+		goto no_merge;
+	if (blk_rq_sectors(req) + bio_sectors(bio) >
+			blk_rq_get_max_sectors(req, blk_rq_pos(req)))
+		goto no_merge;
+
+	/* make sure bio belongs to same range and owner */
+	if (req->biotail &&
+	    (req->biotail->copy.owner != bio->copy.owner ||
+	    req->biotail->copy.orig_src_sector != bio->copy.orig_src_sector ||
+	    req->biotail->copy.orig_dst_sector != bio->copy.orig_dst_sector))
+		goto no_merge;
+
+	req->biotail->bi_next = bio;
+	req->biotail = bio;
+	req->__data_len += bio->bi_iter.bi_size;
+	req->nr_phys_segments = segments + 1;
+
+	blk_account_io_start(req, false);
+	return true;
+no_merge:
+	req_set_nomerge(q, req);
+	return false;
+}
+
 /**
  * blk_attempt_plug_merge - try to merge with %current's plugged list
  * @q: request_queue new bio is being queued at
@@ -727,6 +757,9 @@ bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
 		case ELEVATOR_DISCARD_MERGE:
 			merged = bio_attempt_discard_merge(q, rq, bio);
 			break;
+		case ELEVATOR_SCOPY_MERGE:
+			merged = bio_attempt_simple_copy_merge(q, rq, bio);
+			break;
 		default:
 			break;
 		}
@@ -947,6 +980,10 @@ generic_make_request_checks(struct bio *bio)
 		if (!q->limits.max_write_zeroes_sectors)
 			goto not_supported;
 		break;
+	case REQ_OP_SIMPLE_COPY:
+		if (!bdev_simple_copy_offload(q))
+			goto not_supported;
+		break;
 	default:
 		break;
 	}
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429d4378..3d6495aa0305 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -405,3 +405,152 @@ int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 	return ret;
 }
 EXPORT_SYMBOL(blkdev_issue_zeroout);
+/**
+ * blkdev_simple_copy - Copy offload.
+ * @bdev:	Target block device
+ * @sector:	Source start sector
+ * @dest_Sect:	Destination start sector
+ * @nr_sectors:	Number of sectors to be copied
+ * @gfp_mask:	Memory allocation flags (for bio_alloc)
+ *
+ * Description:
+ *  Issues copy offload REQ_OP_SIMPLE_COPY command.
+ */
+int __blkdev_simple_copy(struct block_device *bdev,
+		      sector_t sect, sector_t dest_sect, sector_t nr_sects,
+		      gfp_t gfp_mask, struct bio **biop)
+{
+	struct request_queue *q = bdev_get_queue(bdev);
+	sector_t end_sect = sect + nr_sects;
+	sector_t orig_dst_sect = dest_sect;
+	sector_t orig_src_sect = sect;
+	unsigned int max_copy_sectors;
+	struct bio *bio = NULL;
+	unsigned int chunk;
+
+	if (!bdev_simple_copy_offload(q))
+		return -EOPNOTSUPP;
+
+	if (bdev_read_only(bdev))
+		return -EPERM;
+
+	if (!nr_sects || end_sect > bdev->bd_part->nr_sects)
+		/* Out of range */
+		return -EINVAL;
+
+	max_copy_sectors = min_t(unsigned int, nr_sects,
+				 q->limits.max_simple_copy_sectors);
+	if (max_copy_sectors == 0)
+		return -EOPNOTSUPP;
+
+	while (sect < end_sect) {
+		bio = blk_next_bio(bio, 0, gfp_mask);
+
+		bio->copy.owner = current->pid;
+		bio->copy.orig_src_sector = orig_src_sect;
+		bio->copy.orig_dst_sector = orig_dst_sect;
+		bio->copy.curr_dst_sector = dest_sect;
+
+		chunk = min_t(unsigned int, nr_sects, max_copy_sectors);
+		bio->bi_iter.bi_sector = sect;
+		bio->bi_iter.bi_size = chunk << 9;
+		bio_set_dev(bio, bdev);
+		bio_set_op_attrs(bio, REQ_OP_SIMPLE_COPY, 0);
+
+		/* This may take a while, so be nice to others */
+		cond_resched();
+		pr_info("%20s src_sector %10llu dst_sector %10llu len %10u\n",
+			__func__, sect, dest_sect, chunk << 9);
+		sect += chunk;
+		dest_sect += chunk;
+	}
+
+	*biop = bio;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(__blkdev_simple_copy);
+
+/**
+ * blkdev_simple_copy - Copy offload.
+ * @bdev:	Target block device
+ * @sector:	Source start sector
+ * @dest_Sect:	Destination start sector
+ * @nr_sectors: Number of sectors to be copied
+ * @gfp_mask:	Memory allocation flags (for bio_alloc)
+ *
+ * Description:
+ * 		Issues copy offload REQ_OP_SIMPLE_COPY command.
+ */
+int blkdev_simple_copy(struct block_device *bdev,
+		      sector_t sect, sector_t dest_sect, sector_t nr_sect,
+		      gfp_t gfp_mask)
+{
+	struct bio *bio = NULL;
+	struct blk_plug plug;
+	int ret;
+
+	blk_start_plug(&plug);
+	ret = __blkdev_simple_copy(bdev, sect, dest_sect, nr_sect, GFP_KERNEL,
+				   &bio);
+	if (!ret && bio) {
+		ret = submit_bio_wait(bio);
+		if (ret == -EOPNOTSUPP)
+			ret = 0;
+		bio_put(bio);
+	}
+	blk_finish_plug(&plug);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(blkdev_simple_copy);
+
+/*
+ * BLKSIMPLECOPY ioctl processing.
+ * Called from blkdev_ioctl.
+ */
+int blkdev_simple_copy_ioctl(struct block_device *bdev, fmode_t mode,
+			     unsigned int cmd, unsigned long arg)
+{
+	void __user *argp = (void __user *)arg;
+	struct request_queue *q;
+	struct blk_simple_copy c;
+
+	if (!argp)
+		return -EINVAL;
+
+	if (!(mode & FMODE_WRITE))
+		return -EBADF;
+
+	q = bdev_get_queue(bdev);
+	if (!q)
+		return -ENXIO;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (!(mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (copy_from_user(&c, (void __user *)arg, sizeof(c)))
+		return -EFAULT;
+
+	if ((c.src_offset & 511) || (c.dst_offset & 511) || (c.len & 511))
+		return -EINVAL;
+
+	c.src_offset >>= 9;
+	c.dst_offset >>= 9;
+	c.len >>= 9;
+
+	if (c.src_offset + c.len > (i_size_read(bdev->bd_inode) >> 9))
+		return -EINVAL;
+
+	if (c.dst_offset + c.len > (i_size_read(bdev->bd_inode) >> 9))
+		return -EINVAL;
+
+	if (c.src_offset + c.len > c.dst_offset)
+		return -EINVAL;
+
+	return blkdev_simple_copy(bdev, c.src_offset, c.dst_offset, c.len,
+				  GFP_KERNEL);
+}
+EXPORT_SYMBOL_GPL(blkdev_simple_copy_ioctl);
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 48e6725b32ee..13c3f47e6cbf 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -116,6 +116,20 @@ static struct bio *blk_bio_write_zeroes_split(struct request_queue *q,
 	return bio_split(bio, q->limits.max_write_zeroes_sectors, GFP_NOIO, bs);
 }
 
+static struct bio *blk_bio_simple_copy_split(struct request_queue *q,
+		struct bio *bio, struct bio_set *bs, unsigned *nsegs)
+{
+	*nsegs = 1;
+
+	if (!q->limits.max_simple_copy_sectors)
+		return NULL;
+
+	if (bio_sectors(bio) <= q->limits.max_simple_copy_sectors)
+		return NULL;
+
+	return bio_split(bio, q->limits.max_simple_copy_sectors, GFP_NOIO, bs);
+}
+
 static struct bio *blk_bio_write_same_split(struct request_queue *q,
 					    struct bio *bio,
 					    struct bio_set *bs,
@@ -300,6 +314,10 @@ void __blk_queue_split(struct request_queue *q, struct bio **bio,
 	case REQ_OP_SECURE_ERASE:
 		split = blk_bio_discard_split(q, *bio, &q->bio_split, nr_segs);
 		break;
+	case REQ_OP_SIMPLE_COPY:
+		split = blk_bio_simple_copy_split(q, *bio, &q->bio_split,
+				nr_segs);
+		break;
 	case REQ_OP_WRITE_ZEROES:
 		split = blk_bio_write_zeroes_split(q, *bio, &q->bio_split,
 				nr_segs);
@@ -367,6 +385,7 @@ unsigned int blk_recalc_rq_segments(struct request *rq)
 	case REQ_OP_DISCARD:
 	case REQ_OP_SECURE_ERASE:
 	case REQ_OP_WRITE_ZEROES:
+	case REQ_OP_SIMPLE_COPY:
 		return 0;
 	case REQ_OP_WRITE_SAME:
 		return 1;
@@ -706,11 +725,29 @@ static inline bool blk_discard_mergable(struct request *req)
 	return false;
 }
 
+/*
+ * Two cases of handling SIMPLE_COPY merge:
+ * If max_simple_copy_segments > 1, the driver takes every bio
+ * as a range and send them to controller together. The ranges
+ * needn't to be contiguous.
+ * Otherwise, the bios/requests will be handled as same as
+ * others which should be contiguous.
+ */
+static inline bool blk_simple_copy_mergeable(struct request *req)
+{
+	if (req_op(req) == REQ_OP_SIMPLE_COPY &&
+	    queue_max_simple_copy_segments(req->q) > 1)
+		return true;
+	return false;
+}
+
 static enum elv_merge blk_try_req_merge(struct request *req,
 					struct request *next)
 {
 	if (blk_discard_mergable(req))
 		return ELEVATOR_DISCARD_MERGE;
+	else if (blk_simple_copy_mergeable(req))
+		return ELEVATOR_SCOPY_MERGE;
 	else if (blk_rq_pos(req) + blk_rq_sectors(req) == blk_rq_pos(next))
 		return ELEVATOR_BACK_MERGE;
 
@@ -797,6 +834,8 @@ static struct request *attempt_merge(struct request_queue *q,
 
 	if (!blk_discard_mergable(req))
 		elv_merge_requests(q, req, next);
+	if (!blk_simple_copy_mergeable(req))
+		elv_merge_requests(q, req, next);
 
 	/*
 	 * 'next' is going away, so update stats accordingly
@@ -887,6 +926,8 @@ enum elv_merge blk_try_merge(struct request *rq, struct bio *bio)
 {
 	if (blk_discard_mergable(rq))
 		return ELEVATOR_DISCARD_MERGE;
+	else if (blk_simple_copy_mergeable(rq))
+		return ELEVATOR_SCOPY_MERGE;
 	else if (blk_rq_pos(rq) + blk_rq_sectors(rq) == bio->bi_iter.bi_sector)
 		return ELEVATOR_BACK_MERGE;
 	else if (blk_rq_pos(rq) - bio_sectors(bio) == bio->bi_iter.bi_sector)
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index c9d183d6c499..a9588e64cf20 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -249,6 +249,8 @@ bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 		return true;
 	case ELEVATOR_DISCARD_MERGE:
 		return bio_attempt_discard_merge(q, rq, bio);
+	case ELEVATOR_SCOPY_MERGE:
+		return bio_attempt_simple_copy_merge(q, rq, bio);
 	default:
 		return false;
 	}
@@ -288,6 +290,9 @@ bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 		case ELEVATOR_DISCARD_MERGE:
 			merged = bio_attempt_discard_merge(q, rq, bio);
 			break;
+		case ELEVATOR_SCOPY_MERGE:
+			merged = bio_attempt_simple_copy_merge(q, rq, bio);
+			break;
 		default:
 			continue;
 		}
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 6bd1e3b082d8..3895a4ceb35d 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -38,6 +38,7 @@ void blk_set_default_limits(struct queue_limits *lim)
 {
 	lim->max_segments = BLK_MAX_SEGMENTS;
 	lim->max_discard_segments = 1;
+	lim->max_simple_copy_segments = 1;
 	lim->max_integrity_segments = 0;
 	lim->seg_boundary_mask = BLK_SEG_BOUNDARY_MASK;
 	lim->virt_boundary_mask = 0;
@@ -76,6 +77,7 @@ void blk_set_stacking_limits(struct queue_limits *lim)
 	/* Inherit limits from component devices */
 	lim->max_segments = USHRT_MAX;
 	lim->max_discard_segments = USHRT_MAX;
+	lim->max_simple_copy_segments = USHRT_MAX;
 	lim->max_hw_sectors = UINT_MAX;
 	lim->max_segment_size = UINT_MAX;
 	lim->max_sectors = UINT_MAX;
@@ -231,6 +233,18 @@ void blk_queue_max_discard_sectors(struct request_queue *q,
 }
 EXPORT_SYMBOL(blk_queue_max_discard_sectors);
 
+/**
+ * blk_queue_max_simple_copy_sectors - set max sectors for a single discard
+ * @q:  the request queue for the device
+ * @max_discard_sectors: maximum number of sectors to discard
+ **/
+void blk_queue_max_simple_copy_sectors(struct request_queue *q,
+		unsigned int max_simple_copy_sectors)
+{
+	q->limits.max_simple_copy_sectors = max_simple_copy_sectors;
+}
+EXPORT_SYMBOL(blk_queue_max_simple_copy_sectors);
+
 /**
  * blk_queue_max_write_same_sectors - set max sectors for a single write same
  * @q:  the request queue for the device
@@ -442,6 +456,23 @@ void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)
 }
 EXPORT_SYMBOL(blk_limits_io_opt);
 
+/**
+ * blk_queue_max_simple_copy_segments - set max segments for simple_copy
+ * requests
+ * @q:  the request queue for the device
+ * @max_segments:  max number of segments
+ *
+ * Description:
+ *    Enables a low level driver to set an upper limit on the number of
+ *    segments in a simple copy request.
+ **/
+void blk_queue_max_simple_copy_segments(struct request_queue *q,
+		unsigned short max_segments)
+{
+	q->limits.max_simple_copy_segments = max_segments;
+}
+EXPORT_SYMBOL_GPL(blk_queue_max_simple_copy_segments);
+
 /**
  * blk_queue_io_opt - set optimal request size for the queue
  * @q:	the request queue for the device
@@ -505,6 +536,8 @@ int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 					b->max_write_same_sectors);
 	t->max_write_zeroes_sectors = min(t->max_write_zeroes_sectors,
 					b->max_write_zeroes_sectors);
+	t->max_simple_copy_sectors = min(t->max_simple_copy_sectors,
+				       b->max_simple_copy_sectors);
 	t->bounce_pfn = min_not_zero(t->bounce_pfn, b->bounce_pfn);
 
 	t->seg_boundary_mask = min_not_zero(t->seg_boundary_mask,
@@ -515,6 +548,8 @@ int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 	t->max_segments = min_not_zero(t->max_segments, b->max_segments);
 	t->max_discard_segments = min_not_zero(t->max_discard_segments,
 					       b->max_discard_segments);
+	t->max_simple_copy_segments = min_not_zero(t->max_simple_copy_segments,
+						 b->max_simple_copy_segments);
 	t->max_integrity_segments = min_not_zero(t->max_integrity_segments,
 						 b->max_integrity_segments);
 
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index b82736c781c5..21cb05f49a06 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -218,6 +218,13 @@ static ssize_t queue_write_zeroes_max_show(struct request_queue *q, char *page)
 		(unsigned long long)q->limits.max_write_zeroes_sectors << 9);
 }
 
+static ssize_t queue_simple_copy_max_sectors_show(struct request_queue *q,
+						  char *page)
+{
+	return sprintf(page, "%llu\n",
+		       (unsigned long long)q->limits.max_simple_copy_sectors);
+}
+
 static ssize_t
 queue_max_sectors_store(struct request_queue *q, const char *page, size_t count)
 {
@@ -542,6 +549,11 @@ static struct queue_sysfs_entry queue_ra_entry = {
 	.store = queue_ra_store,
 };
 
+static struct queue_sysfs_entry queue_max_simple_copy_sectors_entry = {
+	.attr = {.name = "max_simple_copy_sectors", .mode = 0444 },
+	.show = queue_simple_copy_max_sectors_show,
+};
+
 static struct queue_sysfs_entry queue_max_sectors_entry = {
 	.attr = {.name = "max_sectors_kb", .mode = 0644 },
 	.show = queue_max_sectors_show,
@@ -732,6 +744,7 @@ static struct attribute *queue_attrs[] = {
 	&queue_requests_entry.attr,
 	&queue_ra_entry.attr,
 	&queue_max_hw_sectors_entry.attr,
+	&queue_max_simple_copy_sectors_entry.attr,
 	&queue_max_sectors_entry.attr,
 	&queue_max_segments_entry.attr,
 	&queue_max_discard_segments_entry.attr,
diff --git a/block/blk.h b/block/blk.h
index ed347f7a97b1..d3c699b38c3f 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -170,6 +170,8 @@ bool bio_attempt_back_merge(struct request *req, struct bio *bio,
 		unsigned int nr_segs);
 bool bio_attempt_discard_merge(struct request_queue *q, struct request *req,
 		struct bio *bio);
+bool bio_attempt_simple_copy_merge(struct request_queue *q, struct request *req,
+		struct bio *bio);
 bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
 		unsigned int nr_segs, struct request **same_queue_rq);
 
diff --git a/block/ioctl.c b/block/ioctl.c
index 15a0eb80ada9..cc0ec2bff674 100644
--- a/block/ioctl.c
+++ b/block/ioctl.c
@@ -537,6 +537,8 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 		return put_uint(arg, bdev_zone_sectors(bdev));
 	case BLKGETNRZONES:
 		return put_uint(arg, blkdev_nr_zones(bdev));
+	case BLKSIMPLECOPY:
+		return blkdev_simple_copy_ioctl(bdev, mode, cmd, arg);
 	case HDIO_GETGEO:
 		return blkdev_getgeo(bdev, argp);
 	case BLKRAGET:
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 3cdb84cdc488..945fc9c304fd 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -64,7 +64,8 @@ static inline bool bio_has_data(struct bio *bio)
 	    bio->bi_iter.bi_size &&
 	    bio_op(bio) != REQ_OP_DISCARD &&
 	    bio_op(bio) != REQ_OP_SECURE_ERASE &&
-	    bio_op(bio) != REQ_OP_WRITE_ZEROES)
+	    bio_op(bio) != REQ_OP_WRITE_ZEROES &&
+	    bio_op(bio) != REQ_OP_SIMPLE_COPY)
 		return true;
 
 	return false;
@@ -75,7 +76,8 @@ static inline bool bio_no_advance_iter(struct bio *bio)
 	return bio_op(bio) == REQ_OP_DISCARD ||
 	       bio_op(bio) == REQ_OP_SECURE_ERASE ||
 	       bio_op(bio) == REQ_OP_WRITE_SAME ||
-	       bio_op(bio) == REQ_OP_WRITE_ZEROES;
+	       bio_op(bio) == REQ_OP_WRITE_ZEROES ||
+	       bio_op(bio) == REQ_OP_SIMPLE_COPY;
 }
 
 static inline bool bio_mergeable(struct bio *bio)
@@ -178,14 +180,15 @@ static inline unsigned bio_segments(struct bio *bio)
 	struct bvec_iter iter;
 
 	/*
-	 * We special case discard/write same/write zeroes, because they
-	 * interpret bi_size differently:
+	 * We special case discard/write same/write zeroes/simple copy,
+	 * because they interpret bi_size differently:
 	 */
 
 	switch (bio_op(bio)) {
 	case REQ_OP_DISCARD:
 	case REQ_OP_SECURE_ERASE:
 	case REQ_OP_WRITE_ZEROES:
+	case REQ_OP_SIMPLE_COPY:
 		return 0;
 	case REQ_OP_WRITE_SAME:
 		return 1;
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index d688b96d1d63..0c6b430c7817 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -137,6 +137,15 @@ static inline void bio_issue_init(struct bio_issue *issue,
 			((u64)size << BIO_ISSUE_SIZE_SHIFT));
 }
 
+
+/* New bio structure representing information related Simple Copy. */
+struct bio_simple_copy {
+	pid_t		owner;		/* owner of this bio */
+	sector_t	orig_src_sector;/* original source start sector */
+	sector_t	orig_dst_sector;/* original destination start sector */
+	sector_t	curr_dst_sector;/* current destination sector */
+};
+
 /*
  * main unit of I/O for the block layer and lower layers (ie drivers and
  * stacking drivers)
@@ -179,6 +188,8 @@ struct bio {
 #endif
 	};
 
+	/* XXX: add CONFIG_BLK_DEV_SCOPY */
+	struct bio_simple_copy	copy;
 	unsigned short		bi_vcnt;	/* how many bio_vec's */
 
 	/*
@@ -290,6 +301,8 @@ enum req_opf {
 	REQ_OP_ZONE_RESET_ALL	= 8,
 	/* write the zero filled sector many times */
 	REQ_OP_WRITE_ZEROES	= 9,
+	/* Copy Offload */
+	REQ_OP_SIMPLE_COPY	= 11,
 
 	/* SCSI passthrough using struct scsi_request */
 	REQ_OP_SCSI_IN		= 32,
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 6032bb740cf4..46fdd6ee651c 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -337,11 +337,13 @@ struct queue_limits {
 	unsigned int		max_write_zeroes_sectors;
 	unsigned int		discard_granularity;
 	unsigned int		discard_alignment;
+	unsigned int		max_simple_copy_sectors;
 
 	unsigned short		logical_block_size;
 	unsigned short		max_segments;
 	unsigned short		max_integrity_segments;
 	unsigned short		max_discard_segments;
+	unsigned short		max_simple_copy_segments;
 
 	unsigned char		misaligned;
 	unsigned char		discard_misaligned;
@@ -622,6 +624,7 @@ struct request_queue {
 
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))
+#define QUEUE_FLAG_SIMPLE_COPY	26	/* supports Simple Copy */
 
 void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
@@ -640,6 +643,8 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_discard(q)	test_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)
 #define blk_queue_zone_resetall(q)	\
 	test_bit(QUEUE_FLAG_ZONE_RESETALL, &(q)->queue_flags)
+#define blk_queue_simple_copy(q)	\
+	test_bit(QUEUE_FLAG_SIMPLE_COPY, &(q)->queue_flags)
 #define blk_queue_secure_erase(q) \
 	(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))
 #define blk_queue_dax(q)	test_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)
@@ -997,6 +1002,10 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 		return min(q->limits.max_discard_sectors,
 			   UINT_MAX >> SECTOR_SHIFT);
 
+	if (unlikely(op == REQ_OP_SIMPLE_COPY))
+		return min(q->limits.max_simple_copy_sectors,
+			   UINT_MAX >> SECTOR_SHIFT);
+
 	if (unlikely(op == REQ_OP_WRITE_SAME))
 		return q->limits.max_write_same_sectors;
 
@@ -1030,6 +1039,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 
 	if (!q->limits.chunk_sectors ||
 	    req_op(rq) == REQ_OP_DISCARD ||
+	    req_op(rq) == REQ_OP_SIMPLE_COPY ||
 	    req_op(rq) == REQ_OP_SECURE_ERASE)
 		return blk_queue_get_max_sectors(q, req_op(rq));
 
@@ -1073,11 +1083,15 @@ extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_discard_segments(struct request_queue *,
 		unsigned short);
+extern void blk_queue_max_simple_copy_segments(struct request_queue *q,
+		unsigned short max_simple_copy_segnemts);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
 		unsigned int max_discard_sectors);
 extern void blk_queue_max_write_same_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
+extern void blk_queue_max_simple_copy_sectors(struct request_queue *q,
+		unsigned int max_simple_copy_sectors);
 extern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_same_sectors);
 extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
@@ -1136,6 +1150,15 @@ static inline unsigned short blk_rq_nr_discard_segments(struct request *rq)
 	return max_t(unsigned short, rq->nr_phys_segments, 1);
 }
 
+/*
+ * Number of simple copy segments (or ranges) the driver needs to fill in.
+ * Each simple copy bio merged into a request is counted as one segment.
+ */
+static inline unsigned short blk_rq_nr_simple_copy_segments(struct request *rq)
+{
+	return max_t(unsigned short, rq->nr_phys_segments, 1);
+}
+
 extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
 extern void blk_dump_rq_flags(struct request *, char *);
 extern long nr_blockdev_pages(void);
@@ -1225,7 +1248,15 @@ extern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 		unsigned flags);
 extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned flags);
-
+extern int blkdev_simple_copy_ioctl(struct block_device *bdev, fmode_t mode,
+			     unsigned int cmd, unsigned long arg);
+extern int blkdev_simple_copy(struct block_device *bdev,
+		      sector_t sector, sector_t dest_sect, sector_t nr_sectors,
+		      gfp_t gfp_mask);
+extern int __blkdev_simple_copy(struct block_device *bdev,
+				sector_t sect, sector_t dest_sect,
+				sector_t nr_sect, gfp_t gfp_mask,
+				struct bio **bio);
 static inline int sb_issue_discard(struct super_block *sb, sector_t block,
 		sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
 {
@@ -1287,6 +1318,12 @@ static inline unsigned short queue_max_discard_segments(const struct request_que
 	return q->limits.max_discard_segments;
 }
 
+static inline unsigned short
+queue_max_simple_copy_segments(const struct request_queue *q)
+{
+	return q->limits.max_simple_copy_segments;
+}
+
 static inline unsigned int queue_max_segment_size(const struct request_queue *q)
 {
 	return q->limits.max_segment_size;
@@ -1457,6 +1494,14 @@ static inline sector_t bdev_zone_sectors(struct block_device *bdev)
 	return 0;
 }
 
+static inline unsigned int bdev_simple_copy_offload(struct request_queue *q)
+{
+	if (q)
+		return q->limits.max_simple_copy_sectors;
+
+	return 0;
+}
+
 static inline int queue_dma_alignment(const struct request_queue *q)
 {
 	return q ? q->dma_alignment : 511;
diff --git a/include/linux/elevator.h b/include/linux/elevator.h
index 901bda352dcb..b9f43a0c8dd5 100644
--- a/include/linux/elevator.h
+++ b/include/linux/elevator.h
@@ -21,6 +21,7 @@ enum elv_merge {
 	ELEVATOR_FRONT_MERGE	= 1,
 	ELEVATOR_BACK_MERGE	= 2,
 	ELEVATOR_DISCARD_MERGE	= 3,
+	ELEVATOR_SCOPY_MERGE	= 4,
 };
 
 struct blk_mq_alloc_data;
diff --git a/include/uapi/linux/fs.h b/include/uapi/linux/fs.h
index 59c71fa8c553..f6460df9b77d 100644
--- a/include/uapi/linux/fs.h
+++ b/include/uapi/linux/fs.h
@@ -118,6 +118,19 @@ struct fsxattr {
 	unsigned char	fsx_pad[8];
 };
 
+/**
+ * struct blk_simple_copy - BLKSIMPLECOPY ioctl request
+ *
+ * @src_offset: source start offset
+ * @dest_offset: destination start offset
+ * @len: length of the data to be copied
+ */
+struct blk_simple_copy {
+	__u64		src_offset;	/* source offset */
+	__u64		dst_offset;     /* destination offset */
+	__u64		len;		/* total transfer length */
+};
+
 /*
  * Flags for the fsx_xflags field
  */
@@ -181,6 +194,7 @@ struct fsxattr {
 #define BLKSECDISCARD _IO(0x12,125)
 #define BLKROTATIONAL _IO(0x12,126)
 #define BLKZEROOUT _IO(0x12,127)
+#define BLKSIMPLECOPY	_IOR(0x12, 133, struct blk_simple_copy)
 /*
  * A jump here: 130-131 are reserved for zoned block devices
  * (see uapi/linux/blkzoned.h)
-- 
2.17.0



